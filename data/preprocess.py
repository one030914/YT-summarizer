# YouTube ç•™è¨€é è™•ç† + å­˜æª”ç‰ˆ
import re
import emoji
import langid
import jieba
from bs4 import BeautifulSoup
from opencc import OpenCC
import unicodedata
import pandas as pd
from pathlib import Path
import re
from itertools import groupby

# è¨­å®šåªè¾¨è­˜ä¸­æ–‡å’Œè‹±æ–‡ï¼Œé¿å…éåº¦åˆ†é¡
langid.set_languages(['zh', 'en'])

# åˆå§‹åŒ–å·¥å…·
cc = OpenCC('s2t')  # ç°¡é«”è½‰ç¹é«” (Simplified Chinese to Traditional Chinese)

# ===== åŸºæœ¬é è™•ç†å‡½æ•¸ =====

def clean_html(text):
    """ç§»é™¤ HTML æ¨™ç±¤"""
    # ç¢ºä¿è¼¸å…¥ç‚ºå­—ä¸²
    if not isinstance(text, str):
        return ''
    return BeautifulSoup(text, "html.parser").get_text()

# ===== é ç·¨è­¯æ­£å‰‡ =====

URL_RE = re.compile(r'\b(?:https?://|www\.)\S+\b')
# ç§»é™¤å„ç¨®URLï¼URI  
SPECIAL_RE = re.compile(r'[^a-zA-Z0-9\u4e00-\u9fa5\u3040-\u309F\u30A0-\u30FF\u2018\u2019\s\.,!%?]')
# éæ¿¾é™¤ä¸­è‹±æ•¸ã€ä¸­æ–‡ã€æ—¥æ–‡ï¼ˆå¹³å‡å\u3040-\u309Fã€ç‰‡å‡å\u30A0-\u30FFï¼‰ã€ç©ºç™½åŠã€Œ.,!?ã€å¤–çš„ç‰¹æ®Šç¬¦è™Ÿ
WHITESPACE_RE = re.compile(r'\s+') 
# åˆä½µé€£çºŒç©ºç™½ç‚ºå–®ä¸€ç©ºæ ¼  
TIME_RE = re.compile(r'\b\d{1,2}:\d{2}(?::\d{2})?\b')
# å»é™¤ mm:ss æˆ– hh:mm:ss æ ¼å¼æ™‚é–“  
ELLIPSIS_RE = re.compile(r'(?:â€¦|\.{3})')
# ä¿è­·çœç•¥è™Ÿï¼ˆâ€¦ æˆ– ...ï¼‰ä»¥ä¾¿é‚„åŸ  
DUP_PUNC_RE = re.compile(r'([^\w\s])\1+')
# å£“ç¸®é‡è¤‡æ¨™é»æˆå–®ä¸€ç¬¦è™Ÿ
KANA_RE = re.compile(r'[ã-ã‚“ã‚¡-ãƒ³]')
# å¹³å‡å/ç‰‡å‡ååµæ¸¬
CHINESE_RE = re.compile(r'[\u4E00-\u9FFF\u3400-\u4DBF]')
# ä¸­æ–‡å­—ï¼ˆCJK çµ±ä¸€æ¼¢å­—ï¼‰ç¯„åœ
TIME_PATTERN = re.compile(r'(\d{1,2})/(\d{1,2})\s*(ä¸Šåˆ|ä¸‹åˆ)\s*(\d{1,2}):(\d{2})')
# æŠ“æ—¥æœŸæ™‚é–“æ ¼å¼

# ===== CCè½‰æ›ä¿®æ­£è¡¨ =====

CUSTOM_FIXES = {
    "å–«": "åƒ", 
    "çº”": "æ‰",
    "é¬¥å…§": "æŠ–å…§",
    "å‚‘å“¥": "æ°å“¥"
}

# ===== å…¨å½¢åŠå½¢è£œå……è¡¨ =====

FULL2HALF_CUSTOM = str.maketrans({
    'â€™': "'",'â€˜': "'"
})

# ===== é è™•ç†å­å‡½æ•¸ =====

def remove_emoji(text):
    """ç§»é™¤è¡¨æƒ…ç¬¦è™Ÿ"""
    return emoji.replace_emoji(text if isinstance(text, str) else '', replace='')

def remove_url(text):
    """ç§»é™¤ç¶²å€"""
    return URL_RE.sub('', text if isinstance(text, str) else '')

def remove_special_chars(text):
    """ç§»é™¤ç‰¹æ®Šç¬¦è™Ÿï¼Œåªç•™ä¸­è‹±æ–‡ã€æ•¸å­—å’ŒåŸºæœ¬æ¨™é»"""
    return SPECIAL_RE.sub('', text if isinstance(text, str) else '')

def remove_timestamps(text):
    """ç§»é™¤ YouTube ç•™è¨€ä¸­çš„æ™‚é–“è»¸æ¨™è¨˜ (mm:ss æˆ– h:mm:ss)"""
    return TIME_RE.sub('', text if isinstance(text, str) else '')

def normalize_fullwidth_to_halfwidth(text):
    # å…¨å½¢â†’åŠå½¢è£œå……å­—åº«
    t = unicodedata.normalize('NFKC', text if isinstance(text, str) else '')
    # è£œè‡ªè¨‚æ˜ å°„
    return t.translate(FULL2HALF_CUSTOM)

def remove_extra_whitespace(text):
    """å»é™¤å¤šé¤˜ç©ºç™½"""
    return WHITESPACE_RE.sub(' ', text if isinstance(text, str) else '').strip()

def remove_repeated_punct(text: str) -> str:
    # 1. ç”¨ä½”ä½ç¬¦ä¿è­·æ‰€æœ‰çœç•¥è™Ÿ
    ellipses = ELLIPSIS_RE.findall(text)
    placeholder = '<ELLIPSIS>'
    text = ELLIPSIS_RE.sub(placeholder, text)
    # 2. æŠŠå…¶ä»–ã€ŒåŒä¸€å€‹å­—å…ƒé€£ 2 æ¬¡ä»¥ä¸Šã€éƒ½ç¸®æˆ 1 æ¬¡
    text = DUP_PUNC_RE.sub(r'\1', text)
    # 3. é‚„åŸæ‰€æœ‰çœç•¥è™Ÿ
    for _ in ellipses:
        text = text.replace(placeholder, '...')  # æˆ–ç”¨ 'â€¦' è¦–ä½ æƒ³ç”¨å“ªç¨®
    return text

def japanese_ratio(text: str) -> float:
    # å›å‚³æ–‡å­—ä¸­æ—¥æ–‡ï¼ˆå¹³å‡å+ç‰‡å‡åï¼‰å­—å…ƒä½”å…¨éƒ¨éç©ºç™½å­—å…ƒçš„æ¯”ä¾‹
    chars = [c for c in text if not c.isspace()]
    if not chars:
        return 0.0
    jp_count = sum(1 for c in chars if KANA_RE.match(c))
    return jp_count / len(chars)

def remove_chinese_if_mostly_japanese(text: str, threshold: float = 0.15) -> str:
    # åªæœ‰ç•¶æ—¥æ–‡å­—å…ƒä½”æ¯” > threshold æ™‚ï¼Œæ‰ç§»é™¤æ¼¢å­—
    return CHINESE_RE.sub('', text) if japanese_ratio(text) > threshold else text

def collapse_repeated_words(text: str, min_repeat: int = 3) -> str:
    # æŠŠé€£çºŒå‡ºç¾ min_repeat æ¬¡ä»¥ä¸Šçš„åŒä¸€è©ï¼Œå£“ç¸®æˆå–®ä¸€è©ã€‚
    tokens = jieba.lcut(text)
    collapsed = []
    for w, grp in groupby(tokens):
        length = sum(1 for _ in grp)
        if length >= min_repeat:
            collapsed.append(w)
        else:
            collapsed.extend([w] * length)
    return ''.join(collapsed)

def to_chinese_datetime(match):
    # æ—¥æœŸæ™‚é–“è½‰ç‚ºä¸­æ–‡æ•˜è¿°
    month, day, meridiem, hour, minute = match.groups()
    minute_text = "æ•´" if minute == "00" else f"{int(minute)}åˆ†"
    return f"{int(month)}æœˆ{int(day)}æ—¥{meridiem}{int(hour)}é»{minute_text}"

def convert_all_datetimes(text: str) -> str:
    return TIME_PATTERN.sub(to_chinese_datetime, text)

def clean_text(text):
    """åŸ·è¡ŒåŸºæœ¬æ¸…æ´—æµç¨‹"""
    # è™•ç† NaN æˆ–éå­—ä¸²
    text = text if isinstance(text, str) else ''
    steps = [clean_html, normalize_fullwidth_to_halfwidth, expand_english_contractions, remove_chinese_if_mostly_japanese, convert_all_datetimes, remove_timestamps, remove_emoji, remove_url, remove_special_chars, remove_repeated_punct, collapse_repeated_words, remove_extra_whitespace]
    for fn in steps:
        text = fn(text)
    return text

# ===== ä¸­æ–‡æ­£è¦åŒ– =====

def remove_chinese_particles(text):
    """ç§»é™¤ä¸­æ–‡èªåŠ©è©"""
    particles = ["å•Šå•Š","å‘€","é˜¿é˜¿","å§","å˜›","å•¦","å”„","å–”","å™¢","å“¦","å’§","å“‡!","å“‡","å˜","å“©","å“ˆå“ˆ","å˜¿","å”·","å’¦","å”‰","å“","å—¨","å—š","å—¯","å—šå–”","å™“","å–”è€¶","å˜¶","å’»","ç—¾","å“¼","å“¼å“¼","å˜–","å˜–å˜–","å˜¿å˜¿","å–”å”·","å‘ƒ","å‘ƒâ€¦","å—¯å“¼","æ©å“¼","å¼","å—šå‘¼","å””","å™¢è€¶","å™¢ä¸","æ¬¸æ¬¸","å’©","å˜¶â€¦","å“","å’¯","å’¯å’¯","å˜»","å˜»å˜»","å—šå“‡","å“å“","å¶","å™—","å™—é€š","å˜¿å‘¦","å’»å’»","å“§","å”","å”å™“","å’•","å’•åš•","å’š","å’šå’š","å™ ","å™ å™ ","å“","å“ç•¶","å’”","å’”åš“","å•ª","å•ªå•ª","å˜­","ç °","å–”è«"]

    for p in particles:
        text = text.replace(p, '')
    return text

def normalize_chinese(text):
    """ä¸­æ–‡æ­£è¦åŒ–ï¼šç°¡è½‰ç¹ã€å»èªåŠ©è©"""
    text = cc.convert(text)
    return remove_chinese_particles(text)

def cc_convert_with_fixes(text: str) -> str:
    for wrong, right in CUSTOM_FIXES.items():
        text = text.replace(wrong, right)
    return text

# ===== è‹±æ–‡æ­£è¦åŒ– =====

def expand_english_contractions(text):
    """å±•é–‹è‹±æ–‡ç¸®å¯«"""
    contractions = {
        "can't": "cannot",
        "won't": "will not",
        "don't": "do not",
        "didn't": "did not",
        "isn't": "is not",
        "aren't": "are not",
        "it's": "it is",
        "i'm": "i am",
        "you're": "you are",
        "they're": "they are",
        "we're": "we are"
    }
    for k, v in contractions.items():
        text = text.replace(k, v)
    return text


def normalize_english(text):
    """è‹±æ–‡æ­£è¦åŒ–ï¼šå°å¯«åŒ–ã€å±•é–‹ç¸®å¯«"""
    text = text.lower()
    return expand_english_contractions(text)

# ===== å–®å‰‡é è™•ç† =====

def preprocess_comment(raw_comment):
    """å–®å‰‡ç•™è¨€å®Œæ•´é è™•ç†ï¼Œä¸å›å‚³ confidence"""
    # ç¢ºä¿ç‚ºå­—ä¸²
    raw_comment = raw_comment if isinstance(raw_comment, str) else ''
    raw_comment = remove_chinese_if_mostly_japanese(raw_comment)
    cleaned = clean_text(raw_comment)
    if cleaned == '':
        return {"text": "", "language": "unknown"}
    if japanese_ratio(cleaned) > 0.85:
        return {"text": cleaned, "language": "unknown"}
    lang = langid.classify(cleaned)[0]

    if lang == 'zh':
        cleaned = cc_convert_with_fixes(normalize_chinese(cleaned))
    elif lang == 'en':
        cleaned = normalize_english(cleaned)
    else:
        lang = 'unknown'

    return {"text": cleaned, "language": lang}

# ===== æ‰¹æ¬¡è™•ç† =====

def batch_preprocess_comments(json_data):
    """ä¿ç•™ä¸­è‹±æ–‡ï¼Œç§»é™¤é‡è¤‡"""
    comments = []
    for entry in json_data:
        raw = entry.get('commentText', '')
        if not isinstance(raw, str):
            continue
        proc = preprocess_comment(raw)
        if proc['language'] in ['zh', 'en', 'unknown']:
            comments.append({
                "åŸå§‹ç•™è¨€": raw,
                "æ¸…ç†å¾Œç•™è¨€": proc['text'],
                "èªè¨€": proc['language']
            })
    df = pd.DataFrame(comments).drop_duplicates(subset=['æ¸…ç†å¾Œç•™è¨€'])
    return df

# ===== ä¸»ç¨‹å¼ =====

if __name__ == "__main__":
    # 1. å¾ CSV è®€å–åŸå§‹ç•™è¨€æ¬„ä½
    in_path = Path('APItoCSV') / 'youtube_comments.csv'
    df_in = pd.read_csv(in_path, encoding='utf-8-sig')
    if 'text' in df_in.columns and 'commentText' not in df_in.columns:
        df_in = df_in.rename(columns={'text': 'commentText'})
    raw_json = df_in[['commentText']].fillna('').to_dict(orient='records')

    # 2. è™•ç†ä¸¦å­˜æª”
    print("ğŸ”µ é–‹å§‹æ‰¹æ¬¡é è™•ç†ç•™è¨€...")
    df_out = batch_preprocess_comments(raw_json)
    print("ğŸŸ¢ é è™•ç†å®Œæˆï¼Œçµæœå‰å¹¾ç­†ï¼š")
    print(df_out.head())

    out_dir = Path('Preprocess')
    out_dir.mkdir(parents=True, exist_ok=True)
    out_path = out_dir / 'cleaned_comments.csv'
    df_out.to_csv(out_path, index=False, encoding='utf-8-sig')
    print(f"âœ… è¼¸å‡ºåˆ°ï¼š{out_path}")