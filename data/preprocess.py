# YouTube ç•™è¨€é è™•ç† + å­˜æª”ç‰ˆ
import re
import unicodedata
import emoji
import langid
import jieba
import jieba.posseg
import pandas as pd
from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning
import warnings
from opencc import OpenCC
import contractions

# åˆå§‹åŒ–è½‰æ›å™¨
cct2s = OpenCC('t2s')
ccs2t = OpenCC('s2t')
warnings.filterwarnings("ignore", category=MarkupResemblesLocatorWarning)

# æ­£å‰‡è¡¨é”å¼
PATTERNS = {
    'url': re.compile(r'\b(?:https?://|www\.)\S+\b'),
    'special': re.compile(r'[^0-9A-Za-z\u3400-\u4DBF\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF\u2018\u2019\'\s\.,!%?â€¦]'),
    'whitespace': re.compile(r'\s+'),
    'time': re.compile(r'\b\d{1,2}:\d{2}(?::\d{2})?\b'),
    'ellipsis': re.compile(r'(?:â€¦|\.{3,6})'),
    'dup_punc': re.compile(r'([^\w\s])\1+'),
    'repeated': re.compile(r'(?P<grp>.+?)\1+')
}

# è‡ªå®šç¾©è½‰æ›è¡¨
CUSTOM_FIXES = {
    "å–«": "åƒ", "çº”": "æ‰", "é¬¥å…§": "æŠ–å…§", "å‚‘å“¥": "æ°å“¥",
    "å­ƒ": "å¨˜", "ç©«": "ç²", "è£": "è£¡", "ä¸‰æ–‡æ²»": "ä¸‰æ˜æ²»", "é¢": "éºµ"
}

# çµå·´è¨­å®š
JIEBA_PROTECTED = ["è‡ºç£"]
STOPWORDS = {"æ˜¯", "çš„", "äº†", "æœ‰", "æ²¡", "åœ¨", "ä¹Ÿ", "éƒ½", "ä¸º", "èƒ½", "ä¸", "å¥½", "åƒ", "å’§", "è¢‚", "è‘—", 
             "å¸Œæœ›", "äºº", "å¬", "æ­Œ", "ä¼š", "åƒ", "è¦", "çœ‹", "çˆ±", "è®©", "æ‹", "åˆ°", "è¯´", "æ²¡æœ‰", "æœ‰", 
             "è®²", "å¤§", "å°", "é¦–æ­Œ", "ä¸åˆ°"}

def clean_text(text):
    """åŸºæœ¬æ–‡æœ¬æ¸…ç†"""
    if not isinstance(text, str):
        return ''
    
    # HTMLæ¸…ç†
    text = BeautifulSoup(text, "html.parser").get_text()
    
    # åŸºæœ¬æ¸…ç†
    text = unicodedata.normalize('NFKC', text)
    text = PATTERNS['url'].sub('', text)
    text = PATTERNS['special'].sub('', text)
    text = PATTERNS['time'].sub('', text)
    text = emoji.replace_emoji(text, replace='')
    
    # è™•ç†é‡è¤‡æ¨™é»
    ellipses = PATTERNS['ellipsis'].findall(text)
    text = PATTERNS['ellipsis'].sub('<ELLIPSIS>', text)
    text = PATTERNS['dup_punc'].sub(r'\1', text)
    for _ in ellipses:
        text = text.replace('<ELLIPSIS>', '...')
    
    # æ¸…ç†ç©ºç™½
    return PATTERNS['whitespace'].sub(' ', text).strip()

class TextNormalizer:
    def __init__(self):
        for word in JIEBA_PROTECTED:
            jieba.add_word(word)
    
    def normalize_chinese(self, text):
        """ä¸­æ–‡æ­£è¦åŒ–"""
        # ç¹è½‰ç°¡
        text = cct2s.convert(text)
        
        # åˆ†è©è™•ç†
        words = []
        for w, flag in jieba.posseg.lcut(text):
            new = PATTERNS['repeated'].sub(r'\g<grp>', w)
            if new == text:
                break
            w = new
            
            if (flag.startswith(('n','v','a')) or flag in ('i','l')) and w not in STOPWORDS:
                words.append(w)
        
        # è™•ç†é‡è¤‡ç‰‡æ®µ
        words = list(dict.fromkeys(words))
        text = ''.join(words)
            
        return text, words
    
    def normalize_english(self, text):
        """è‹±æ–‡æ­£è¦åŒ–"""
        text = text.lower()
        text = re.sub(r'([A-Za-z])\1{2,}', r'\1', text)
        return contractions.fix(text)

def preprocess_comment(raw_comment):
    """å–®å‰‡ç•™è¨€é è™•ç†"""
    cleaned = clean_text(raw_comment)
    
    lang = langid.classify(cleaned)[0]
    normalizer = TextNormalizer()
    tokens = []
    
    if lang == 'zh':
        cleaned, tokens = normalizer.normalize_chinese(cleaned)
        tokens = [ccs2t.convert(w) for w in tokens]
        tokens = [CUSTOM_FIXES.get(w, w) for w in tokens]
        cleaned = ccs2t.convert(cleaned)
    elif lang == 'en':
        cleaned = normalizer.normalize_english(cleaned)
        tokens = []
    else:
        lang = 'unknown'
    
    return {"text": cleaned, "language": lang, "tokens": tokens}

def batch_preprocess_comments(json_data):
    """æ‰¹æ¬¡è™•ç†ç•™è¨€"""
    comments = []
    for entry in json_data:
        rawcomments = entry['åŸç•™è¨€']
        if not isinstance(rawcomments, str):
            continue
        # rawlikecount = str(entry['æŒ‰è®šæ•¸'])
        # rawreplycount = str(entry['å›è¦†æ•¸'])
        proc = preprocess_comment(rawcomments)
        if proc['language'] in ['zh', 'en', 'unknown']:
            comments.append({
                # "èªè¨€": proc['language'],
                # "åŸç•™è¨€": rawcomments,
                # "æŒ‰è®šæ•¸": rawlikecount,
                # "å›è¦†æ•¸": rawreplycount,
                "æ¸…ç†å¾Œç•™è¨€": proc['text'],
                # "çµå·´æ–·è©": ",".join(f"'{w}'" for w in proc.get("tokens", []))
            })
    
    return pd.DataFrame(comments).drop_duplicates(subset=['æ¸…ç†å¾Œç•™è¨€'])

'''
if __name__ == "__main__":
    in_path = out_dir / f'{VIDEO_ID}.csv'
    df_in = pd.read_csv(in_path, encoding='utf-8-sig')
    if 'åŸç•™è¨€' in df_in.columns and 'commentText' not in df_in.columns:
        df_in = df_in.rename(columns={'åŸç•™è¨€': 'commentText'})
    if 'æŒ‰è®šæ•¸' in df_in.columns and 'likecount' not in df_in.columns:
        df_in = df_in.rename(columns={'æŒ‰è®šæ•¸': 'likecount'})
    if 'å›è¦†æ•¸' in df_in.columns and 'replycount' not in df_in.columns:
        df_in = df_in.rename(columns={'å›è¦†æ•¸': 'replycount'})
    
    print("ğŸ”µ é–‹å§‹æ‰¹æ¬¡é è™•ç†ç•™è¨€...")
    df_out = batch_preprocess_comments(df_in[['commentText','likecount','replycount']].fillna('').to_dict(orient='records'))
    print("ğŸŸ¢ é è™•ç†å®Œæˆï¼Œçµæœå‰å¹¾ç­†ï¼š")
    print(df_out.head())
    
    out_path = out_dir / f'{VIDEO_D}.csv'
    df_out.to_csv(out_path, index=False, encoding='utf-8-sig')
    print(f"âœ… è¼¸å‡ºåˆ°ï¼š{out_path}")
'''